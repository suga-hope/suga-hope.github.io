<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.8.0" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.8.0" type="image/png" sizes="32x32"><meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary"><title>Hexo</title><link ref="canonical" href="http://example.com/index.html"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.8.0"><link rel="stylesheet" href="css/custom.css"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"Copy","copySuccess":"Copy Success","copyError":"Copy Error"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">Home</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">Archives</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">Hello Stun</div><div class="header-banner-info__subtitle">An elegant theme for Hexo</div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content content-home" id="content"><section class="postlist"><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/24/kafka-eagle/">在kafka集群中部署Eagle运维监控</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-24</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-25</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>部署Kafka Eagle，完成对kafka集群的基本监控。</p>

        <h1 id="安装配置流程"   >
          <a href="#安装配置流程" class="heading-link"><i class="fas fa-link"></i></a><a href="#安装配置流程" class="headerlink" title="安装配置流程"></a>安装配置流程</h1>
      <p>1、上传安装包到服务器中，解压到&#x2F;export&#x2F;server目录下</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/software/</span><br><span class="line">tar -zxvf kafka-eagle-web-2.0.2-bin.tar.gz -C /export/server/</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/eagle/1.png"></p>
<p>2、设置软连接</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/server/kafka-eagle-web-2.0.2/ /export/server/kafka-eagle</span><br></pre></td></tr></table></div></figure>

<p>3、 配置环境变量:JAVA_HOME 和 KE_HOME</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line"></span><br><span class="line"># JAVA_HOME(之前配置过的，不用配置，会有冲突，可能无法识别)</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"># KE_HOME</span><br><span class="line">export KE_HOME=/export/server/kafka-eagle</span><br><span class="line">export PATH=$PATH:$KE_HOME/bin</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></div></figure>

<p>4、 配置 KafkaEagle</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/kafka-eagle/conf </span><br><span class="line">vi system-config.properties</span><br><span class="line"></span><br><span class="line">#需要更改的地方如下图，改为：</span><br><span class="line">kafka.eagle.zk.cluster.alias=cluster1</span><br><span class="line">cluster1.zk.list=node1:2181,node2:2181,node3:2181</span><br><span class="line">cluster1.kafka.eagle.broker.size=3</span><br><span class="line"></span><br><span class="line">kafka.eagle.url=jdbc:sqlite:/export/data/db/ke.db</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/eagle/2.png"></p>
<p><img src="/../image/eagle/3.png"></p>
<p>5、启动前需要手动创建&#x2F;export&#x2F;data&#x2F;db目录</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /export/data/db</span><br></pre></td></tr></table></div></figure>

<p>6、启动zookeeper</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkall.sh start</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/eagle/5.png"></p>
<p>7、启动kafka集群</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kfkall.sh start</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/eagle/6.png"></p>
<p><img src="/../image/eagle/7.png"></p>
<p>8、启动Eagle</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/export/server/kafka-eagle/bin/ke.sh start</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/eagle/8.png"></p>
<p>9、网页访问Eagle</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> Account:admin </span><br><span class="line"> Password:123456</span><br><span class="line"></span><br><span class="line">http://192.168.88.151:8048</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/eagle/9.png"></p>

        <h1 id="Eagle可视化页面操作"   >
          <a href="#Eagle可视化页面操作" class="heading-link"><i class="fas fa-link"></i></a><a href="#Eagle可视化页面操作" class="headerlink" title="Eagle可视化页面操作"></a>Eagle可视化页面操作</h1>
      <p>访问地址：<a href="http://192.168.88.151:8048,输入账号:`admin`;密码:`123456`,登录成功后可以访问到Dashboard。">http://192.168.88.151:8048,输入账号:`admin`;密码:`123456`,登录成功后可以访问到Dashboard。</a></p>
<p><img src="/../image/eagle/11.png"></p>
<p>1、创建Topic</p>
<p><img src="/../image/eagle/12.png"></p>
<p>2、topic列表</p>
<p><img src="/../image/eagle/13.png"></p>
<p>3、消息拟发送</p>
<p><img src="/../image/eagle/14.png"></p>
<p>4、查看实时状态监控图</p>
<p><img src="/../image/eagle/10.png"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/24/kafka-API%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/">kafka API使用方法</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-24</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-25</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="API-开发：producer-生产者"   >
          <a href="#API-开发：producer-生产者" class="heading-link"><i class="fas fa-link"></i></a><a href="#API-开发：producer-生产者" class="headerlink" title="API 开发：producer 生产者"></a>API 开发：producer 生产者</h1>
      
        <h2 id="生产者api示例"   >
          <a href="#生产者api示例" class="heading-link"><i class="fas fa-link"></i></a><a href="#生产者api示例" class="headerlink" title="生产者api示例"></a>生产者api示例</h2>
      <p>一个正常的生产逻辑需要具备以下几个步骤</p>
<p>(1)配置生产者客户端参数</p>
<p>(2)创建相应的生产者实例</p>
<p>(3)构建待发送的消息</p>
<p>(4)发送消息</p>
<p>(5)关闭生产者实例</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">//首先,引入 maven 依赖</span><br><span class="line">&lt;dependency&gt; </span><br><span class="line">	&lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; </span><br><span class="line">	&lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; </span><br><span class="line">	&lt;version&gt;2.0.0&lt;/version&gt; </span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></div></figure>

<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">//采用默认分区方式将消息散列的发送到各个分区当中</span><br><span class="line">import org.apache.kafka.clients.producer.KafkaProducer; </span><br><span class="line">import org.apache.kafka.clients.producer.Producer; </span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord; </span><br><span class="line">import java.util.Properties; </span><br><span class="line">public class MyProducer &#123; </span><br><span class="line">	public static void main(String[ ] args) throws InterruptedException &#123; </span><br><span class="line">		Properties props = new Properties(); </span><br><span class="line">		//设置 kafka 集群的地址</span><br><span class="line">		props.put(&quot;bootstrap.servers&quot;, &quot;node1:9092,node2:9092,node3:9092&quot;);</span><br><span class="line">		//ack 模式,取值有 0,1,-1(all) , all 是最慢但最安全的，</span><br><span class="line">		//0-&gt;不等响应就继续发（可靠性低），</span><br><span class="line">		//1-&gt;leader会写到本地日志后，然后给响应，producer拿到响应才继续发（follwer还没同步）</span><br><span class="line">		props.put(“acks”, “all”); //--》很重要</span><br><span class="line"></span><br><span class="line">		props.put(“retries”, 3); //失败重试次数-&gt;失败会自动重试（可恢复/不可恢复）--&gt;(有可能会造成数据的乱序)</span><br><span class="line">		props.put(“batch.size”, 10); //数据发送的批次大小提高效率/吞吐量太大会数据延迟</span><br><span class="line">		props.put(&quot;linger.ms&quot;, 10000); //消息在缓冲区保留的时间,超过设置的值就会被提交到服务端</span><br><span class="line">		props.put(&quot;max.request.size&quot;,10); //数据发送请求的最大缓存数</span><br><span class="line">		props.put(“buffer.memory”, 10240); //整个 Producer 用到总内存的大小,如果缓冲区满了会提交数据到服务端</span><br><span class="line">		//buffer.memory 要大于 batch.size,否则会报申请内存不足的错误降低阻塞的可能性</span><br><span class="line">		props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); //key-value序列化器</span><br><span class="line">		props.put(&quot;value.serializer&quot;, 	&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);//字符串最好</span><br><span class="line">		Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); </span><br><span class="line">		for (int i = 0; i &lt; 100; i++) </span><br><span class="line">			producer.send(new ProducerRecord&lt;String, String&gt;(&quot;test&quot;, Integer.toString(i), &quot;dd:&quot;+i)); </span><br><span class="line">        //Thread.sleep(1000000); </span><br><span class="line">        producer.close(); </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>


        <h2 id="必要的参数配置"   >
          <a href="#必要的参数配置" class="heading-link"><i class="fas fa-link"></i></a><a href="#必要的参数配置" class="headerlink" title="必要的参数配置"></a>必要的参数配置</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">//在创建真正的生产者实例前需要配置相应的参数,比如需要连接的 Kafka 集群地址。在 Kafka 生产者客户端 KatkaProducer 中有 3 个参数是必填的。</span><br><span class="line">-bootstrap.servers </span><br><span class="line">-key.serializer </span><br><span class="line">-value.serializer</span><br><span class="line">    </span><br><span class="line">//为了防止参数名字符串书写错误,可以使用如下方式进行设置: </span><br><span class="line">props.setProperty(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,ProducerInterceptorPrefix.class.getName());</span><br><span class="line">props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,&quot;node1:9092,node2:9092&quot;); </span><br><span class="line">props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName()); props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());</span><br></pre></td></tr></table></div></figure>


        <h2 id="发送消息"   >
          <a href="#发送消息" class="heading-link"><i class="fas fa-link"></i></a><a href="#发送消息" class="headerlink" title="发送消息"></a>发送消息</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">//代码示例</span><br><span class="line">import org.apache.kafka.clients.producer.*; </span><br><span class="line">import java.util.Properties; </span><br><span class="line">public class MyProducer &#123; </span><br><span class="line">	public static void main(String[] args) throws InterruptedException &#123; </span><br><span class="line">		Properties props = new Properties(); // Kafka 服务端的主机名和端口号</span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;node1:9092,node2:9092,node3:9092&quot;); </span><br><span class="line">		// 等待所有副本节点的应答</span><br><span class="line">		props.put(&quot;acks&quot;, &quot;all&quot;); // 消息发送最大尝试次数</span><br><span class="line">		props.put(&quot;retries&quot;, 0); // 一批消息处理大小</span><br><span class="line">		props.put(&quot;batch.size&quot;, 16384); // 增加服务端请求延时</span><br><span class="line">		props.put(&quot;linger.ms&quot;, 1); // 发送缓存区内存大小</span><br><span class="line">		props.put(&quot;buffer.memory&quot;, 33554432); // key 序列化</span><br><span class="line">		props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); </span><br><span class="line">		// value 序列化</span><br><span class="line">		props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); 				  			KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;&gt;(props); </span><br><span class="line">		for (int i = 0; i &lt; 50; i++) &#123; </span><br><span class="line">			kafkaProducer.send(new ProducerRecord&lt;String, String&gt;(&quot;test&quot;, &quot;hello&quot; + i), new Callback() &#123; </span><br><span class="line">			@Override </span><br><span class="line">			public void onCompletion(RecordMetadata metadata, Exception exception) &#123; </span><br><span class="line">				if (metadata != null) &#123; </span><br><span class="line">					System.out.println(metadata.partition() + &quot;---&quot; + metadata.offset()); </span><br><span class="line">                &#125; </span><br><span class="line">            &#125; </span><br><span class="line">            &#125;); </span><br><span class="line">        &#125; </span><br><span class="line">        kafkaProducer.close(); </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>


        <h1 id="API-开发：consumer生产者"   >
          <a href="#API-开发：consumer生产者" class="heading-link"><i class="fas fa-link"></i></a><a href="#API-开发：consumer生产者" class="headerlink" title="API 开发：consumer生产者"></a>API 开发：consumer生产者</h1>
      
        <h2 id="消费者Api-示例"   >
          <a href="#消费者Api-示例" class="heading-link"><i class="fas fa-link"></i></a><a href="#消费者Api-示例" class="headerlink" title="消费者Api 示例"></a>消费者Api 示例</h2>
      <p>一个正常的消费逻辑需要具备以下几个步骤: </p>
<p>(1)配置消费者客户端参数</p>
<p>(2)创建相应的消费者实例; </p>
<p>(3)订阅主题; </p>
<p>(4)拉取消息并消费; </p>
<p>(5)提交消费位移 offset;</p>
<p>(6)关闭消费者实例。</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">//消费者实例代码</span><br><span class="line">import org.apache.kafka.clients.consumer.*; </span><br><span class="line">import java.util.Arrays; </span><br><span class="line">import java.util.Properties; </span><br><span class="line">public class MyConsumer &#123; </span><br><span class="line">    public static void main(String[] args) &#123; </span><br><span class="line">		Properties props = new Properties(); </span><br><span class="line">		// 定义 kakfa 服务的地址,不需要将所有 broker 指定上</span><br><span class="line">		props.put(&quot;bootstrap.servers&quot;, &quot;node1:9092&quot;); </span><br><span class="line">		// 指定 consumer group </span><br><span class="line">		props.put(&quot;group.id&quot;, &quot;g1&quot;); </span><br><span class="line">		// 是否自动提交 offset </span><br><span class="line">		props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;); </span><br><span class="line">		// 自动提交 offset 的时间间隔</span><br><span class="line">		props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);</span><br><span class="line">		// key 的反序列化类</span><br><span class="line">		props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); </span><br><span class="line">		// value 的反序列化类</span><br><span class="line">		props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); </span><br><span class="line">		// 如果没有消费偏移量记录,则自动重设为起始 offset:latest, earliest, none</span><br><span class="line">		//Earliest-&gt;目前状态下最前面的一条消息（日志在一定保存时间后会自动清空）</span><br><span class="line">		//none（上次记录的偏移量，如果没有，会抛异常） </span><br><span class="line"></span><br><span class="line">		props.put(&quot;auto.offset.reset&quot;,&quot;earliest&quot;); </span><br><span class="line">		// 定义 consumer </span><br><span class="line">		KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); </span><br><span class="line">		// 消费者订阅的 topic, 可同时订阅多个</span><br><span class="line">		consumer.subscribe(Arrays.asList(&quot;first&quot;, &quot;test&quot;,&quot;test1&quot;)); </span><br><span class="line">		while (true) &#123; </span><br><span class="line">		// 读取数据,读取超时时间为 100ms </span><br><span class="line">			ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); </span><br><span class="line">			for (ConsumerRecord&lt;String, String&gt; record : records) </span><br><span class="line">				System.out.printf(&quot;offset = %d, key = %s, value = %s%n&quot;, 		record.offset(), record.key(), record.value()); </span><br><span class="line">        &#125; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>


        <h2 id="必要参数配置"   >
          <a href="#必要参数配置" class="heading-link"><i class="fas fa-link"></i></a><a href="#必要参数配置" class="headerlink" title="必要参数配置"></a>必要参数配置</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">//也可以使用如下形式:</span><br><span class="line">Properties props = new Properties();</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,brokerList);</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.GROUP_ID_CONFIG,groupid);</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.CLIENT_ID_CONFIG,clientid);</span><br></pre></td></tr></table></div></figure>


        <h2 id="subscribe-订阅主题"   >
          <a href="#subscribe-订阅主题" class="heading-link"><i class="fas fa-link"></i></a><a href="#subscribe-订阅主题" class="headerlink" title="subscribe 订阅主题"></a>subscribe 订阅主题</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//subscribe 有如下重载方法: </span><br><span class="line">//(1)前面两种是通过集合的方式订阅一到多个topic</span><br><span class="line">public void subscribe(Collection&lt;String&gt; topics,ConsumerRebalanceListener listener)</span><br><span class="line">public void subscribe(Collection&lt;String&gt; topics)</span><br><span class="line">//(2)后两种主要是采用正则的方式订阅一到多个topic</span><br><span class="line">public void subscribe(Pattern pattern, ConsumerRebalanceListener listener) </span><br><span class="line">public void subscribe(Pattern pattern)</span><br><span class="line"></span><br><span class="line">//指定集合方式订阅主题</span><br><span class="line">consumer.subscribe(Arrays.asList(topic1)); </span><br><span class="line">consumer subscribe(Arrays.asList(topic2))</span><br></pre></td></tr></table></div></figure>


        <h2 id="assign订阅主题"   >
          <a href="#assign订阅主题" class="heading-link"><i class="fas fa-link"></i></a><a href="#assign订阅主题" class="headerlink" title="assign订阅主题"></a>assign订阅主题</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">消费者不仅可以通过 KafkaConsumer.subscribe() 方法订阅主题,还可直接订阅某些主题的指定分区; </span><br><span class="line"></span><br><span class="line">在 KafkaConsumer 中提供了 assign() 方法来实现这些功能,此方法的具体定义如下: </span><br><span class="line"></span><br><span class="line">public void assign(Collection&lt;TopicPartition&gt; partitions) </span><br><span class="line"></span><br><span class="line">//这个方法只接受参数 partitions,用来指定需要订阅的分区集合。</span><br><span class="line"></span><br><span class="line">示例如下: </span><br><span class="line">consumer.assign(Arrays.asList(new TopicPartition (&quot;tpc_1&quot; , 0),new TopicPartition(“tpc_2”,1))) ;</span><br></pre></td></tr></table></div></figure>


        <h2 id="取消订阅"   >
          <a href="#取消订阅" class="heading-link"><i class="fas fa-link"></i></a><a href="#取消订阅" class="headerlink" title="取消订阅"></a>取消订阅</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">//既然有订阅,那么就有取消订阅; </span><br><span class="line"></span><br><span class="line">可以使用 KafkaConsumer 中的 unsubscribe()方法采取消主题的订阅,这个方法既可以取消通过subscribe( Collection)方式实现的订阅; </span><br><span class="line"></span><br><span class="line">也可以取消通过 subscribe(Pattem)方式实现的订阅,还可以取消通过 assign( Collection)方式实现的订阅。示例码如下: </span><br><span class="line"></span><br><span class="line">consumer.unsubscribe(); </span><br><span class="line"></span><br><span class="line">如果将 subscribe(Collection )或 assign(Collection)集合参数设置为空集合,作用与 unsubscribe()方法相同,如下示例中三行代码的效果相同: </span><br><span class="line"></span><br><span class="line">1.consumer.unsubscribe(  ); </span><br><span class="line">2.consumer.subscribe(new ArrayList&lt;String&gt;(  )) ; </span><br><span class="line">3.consumer.assign(new ArrayList&lt;TopicPartition&gt;(  ));</span><br></pre></td></tr></table></div></figure>


        <h2 id="消息的消费模式"   >
          <a href="#消息的消费模式" class="heading-link"><i class="fas fa-link"></i></a><a href="#消息的消费模式" class="headerlink" title="消息的消费模式"></a>消息的消费模式</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">//Kafka 中的消费是基于拉取模式的。消息的消费一般有两种模式:推送模式和拉取模式。</span><br><span class="line">//推模式是服务端主动将消息推送给消费者,而拉模式是消费者主动向服务端发起请求来拉取消息。</span><br><span class="line"></span><br><span class="line">Kafka 中的消息消费是一个不断轮询的过程,消费者所要做的就是重复地调用 poll() 方法,poll()方法返回的是所订阅的主题(分区)上的一组消息。</span><br><span class="line"></span><br><span class="line">对于 poll ()方法而言,如果某些分区中没有可供消费的消息,那么此分区对应的消息拉取的结果就为空如果订阅的所有分区中都没有可供消费的消息,那么 poll()方法返回为空的消息集; poll () 方法具体定义如下: </span><br><span class="line"></span><br><span class="line">public ConsumerRecords&lt;K, V&gt; poll(final Duration timeout) </span><br><span class="line">    </span><br><span class="line">超时时间参数 timeout , 用来控制 poll() 方法的阻塞时间, 在消费者的缓冲区里没有可用数据时会发生阻塞。如果消费者程序只用来单纯拉取并消费数据,则为了提高吞吐率,可以把 timeout 设置为Long.MAX_VALUE;</span><br></pre></td></tr></table></div></figure>

<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">//消费者消费到的每条消息的类型为 ConsumerRecord</span><br><span class="line">public class ConsumerRecord&lt;K, V&gt; &#123; </span><br><span class="line">	public static final long NO_TIMESTAMP = RecordBatch.NO_TIMESTAMP; </span><br><span class="line">	public static final int NULL_SIZE = -1; </span><br><span class="line">	public static final int NULL_CHECKSUM = -1; </span><br><span class="line">	private final String topic; </span><br><span class="line">	private final int partition; </span><br><span class="line">	private final long offset;</span><br><span class="line">	private final long timestamp; </span><br><span class="line">	private final TimestampType timestampType; </span><br><span class="line">	private final int serializedKeySize; </span><br><span class="line">	private final int serializedValueSize; </span><br><span class="line">	private final Headers headers; </span><br><span class="line">	private final K key; </span><br><span class="line">	private final V value; </span><br><span class="line">	private volatile Long checksum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">//示例代码片段</span><br><span class="line">/**</span><br><span class="line">* 订阅与消费方式2</span><br><span class="line">*/</span><br><span class="line">TopicPartition tp1 = new TopicPartition(&quot;x&quot;, 0);</span><br><span class="line">TopicPartition tp2 = new TopicPartition(&quot;y&quot;, 0);</span><br><span class="line">TopicPartition tp3 = new TopicPartition(&quot;z&quot;, 0);</span><br><span class="line">List&lt;TopicPartition&gt; tps = Arrays.asList(tp1, tp2, tp3);</span><br><span class="line">consumer.assign(tps);</span><br><span class="line">while (true) &#123;</span><br><span class="line">	ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(1000));</span><br><span class="line">	for (TopicPartition tp : tps) &#123;</span><br><span class="line">		List&lt;ConsumerRecord&lt;String, String&gt;&gt; rList = records.records(tp);</span><br><span class="line">		for (ConsumerRecord&lt;String, String&gt; r : rList) &#123;</span><br><span class="line">			r.topic();</span><br><span class="line">			r.partition();</span><br><span class="line">			r.offset();</span><br><span class="line">			r.value();</span><br><span class="line">			//do something to process record.</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>


        <h2 id="指定位移消费"   >
          <a href="#指定位移消费" class="heading-link"><i class="fas fa-link"></i></a><a href="#指定位移消费" class="headerlink" title="指定位移消费"></a>指定位移消费</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">有些时候,我们需要一种更细粒度的掌控,可以让我们从特定的位移处开始拉取消息,而KafkaConsumer 中的 seek() 方法正好提供了这个功能,让我们可以追前消费或回溯消费。</span><br><span class="line"></span><br><span class="line">seek()方法的具体定义如下: </span><br><span class="line"></span><br><span class="line">public void seek(TopicPartiton partition,long offset)</span><br></pre></td></tr></table></div></figure>

<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">//代码示例</span><br><span class="line">// 在调用seek 方法之前，需要先调用一次poll，以分配到分区</span><br><span class="line">consumer.poll(Duration.ofMillis(1000));</span><br><span class="line"></span><br><span class="line">// 获取所分配到的分区信息</span><br><span class="line">Set&lt;TopicPartition&gt; assignment = consumer.assignment();</span><br><span class="line">for (TopicPartition topicPartition : assignment) &#123;</span><br><span class="line">	// 为指定partition 设置读取起始offset</span><br><span class="line">	consumer.seek(topicPartition, 80);</span><br><span class="line">&#125;</span><br><span class="line">// 开始正式消费</span><br><span class="line">while (true) &#123;</span><br><span class="line">	ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(1000));</span><br><span class="line">	for (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">	// do some process</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>


        <h2 id="再均衡监听器"   >
          <a href="#再均衡监听器" class="heading-link"><i class="fas fa-link"></i></a><a href="#再均衡监听器" class="headerlink" title="再均衡监听器"></a>再均衡监听器</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">//代码示例</span><br><span class="line">/**</span><br><span class="line">* 再均衡处理</span><br><span class="line">*/</span><br><span class="line">consumer.subscribe(Collections.singletonList(&quot;tpc_5&quot;), new ConsumerRebalanceListener() &#123;</span><br><span class="line">	// 再均衡开始前和消费者停止读取消息之后，被调用</span><br><span class="line">	@Override</span><br><span class="line">	public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; collection) &#123;</span><br><span class="line">	// store the current offset to db</span><br><span class="line">	&#125;</span><br><span class="line">	// 重新分配到分区后和消费者开始读取消息之前，被调用</span><br><span class="line">	@Override</span><br><span class="line">	public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; collection) &#123;</span><br><span class="line">	// fetch the current offset from db</span><br><span class="line">	&#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></div></figure>


        <h2 id="自动位移提交"   >
          <a href="#自动位移提交" class="heading-link"><i class="fas fa-link"></i></a><a href="#自动位移提交" class="headerlink" title="自动位移提交"></a>自动位移提交</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Kafka 中默认的消费位移的提交方式是自动提交,这个由消费者客户端参数 `enable.auto.commit `配置, 默认值为 true 。</span><br><span class="line"></span><br><span class="line">当然这个默认的自动提交不是每消费一条消息就提交一次,而是定期提交,这个定期的周期时间由客户端参数 `auto.commit.interval.ms` 配置, 默认值为 5 秒, 此参数生效的前提是 enable.</span><br><span class="line">`auto.commit` 参数为 true。</span><br><span class="line"></span><br><span class="line">在默认的方式下,消费者每隔 5 秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在 `poll() `方法的逻辑里完成的,在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交,如果可以,那么就会提交上一次轮询的位移。</span><br><span class="line"></span><br><span class="line">#Kafka 消费的编程逻辑中位移提交是一大难点,自动提交消费位移的方式非常简便,它免去了复杂的位移提交逻辑,让编码更简洁。但随之而来的是重复消费和消息丢失的问题。</span><br></pre></td></tr></table></div></figure>

</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/24/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/">Kafka命令行操作</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-24</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-25</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>Kafka 中提供了许多命令行工具（位于$KAFKA HOME&#x2F;bin 目录下）用于管理集群的变更。</p>

        <h1 id="创建topic"   >
          <a href="#创建topic" class="heading-link"><i class="fas fa-link"></i></a><a href="#创建topic" class="headerlink" title="创建topic"></a>创建topic</h1>
      <p>创建一个topic（主题）。Kafka中所有的消息都是保存在主题中，要生产消息到Kafka，首先必须要有一个确定的主题。</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#基本方式</span><br><span class="line">./kafka-topics.sh --create --topic tpc_1 --partitions 2 --replication-factor 2 --zookeeper node1:2181</span><br><span class="line"></span><br><span class="line">--replication-factor 副本数量</span><br><span class="line">--partitions 分区数量</span><br><span class="line">--topic topic 名称</span><br><span class="line"></span><br><span class="line">#手动指定副本的存储位置</span><br><span class="line">bin/kafka-topics.sh --create --topic tpc_1 --zookeeper node1:2181 --replica-assignment 0:1,1:2</span><br><span class="line">该方式下,命令会自动判断所要创建的 topic 的分区数及副本数</span><br><span class="line"></span><br><span class="line">#bootstrap方式</span><br><span class="line"># 创建名为test的主题</span><br><span class="line">bin/kafka-topics.sh --create --bootstrap-server node1:9092 --topic test</span><br><span class="line"># 查看目前Kafka中的主题</span><br><span class="line">bin/kafka-topics.sh --list --bootstrap-server node1:9092</span><br></pre></td></tr></table></div></figure>

<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">--replica-assignment 不能同时使用--partitions --replication-factor参数指定partition的AR列表，未指定AR列表则会根据负载均衡算法将partition的replica均衡的分布在Kafka集群中。</span><br><span class="line"></span><br><span class="line">--replica-assignment 1:3,2:1,3:2，</span><br><span class="line">#逗号区分不同的partition，</span><br><span class="line">#冒号区别相同partition中的replica，</span><br><span class="line">#partition-0的AR=[1,3]，partition-1的AR=[2,1]，partition-2的AR=[3,2]。</span><br><span class="line"></span><br><span class="line">Eg：testMcdull222AR列表计算出来时--replica-assignment 2:3,1:3,1:2 。数字指的是broker的ID号</span><br></pre></td></tr></table></div></figure>

<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">--replica-assignment 参数一般不由用户指定，由Kafka默认分配算法保证，有两个原则：</span><br><span class="line"></span><br><span class="line">（1）使Topic的所有Partition Replica能够均匀地分配至各个Kafka Broker（负载均衡）；</span><br><span class="line">（2）Partition 内的replica能够均匀地分配在不同Kafka Broker。</span><br><span class="line"></span><br><span class="line">#如果Partition的第一个Replica分配至某一个Kafka Broker，那么这个Partition的其它Replica则需要分配至其它的Kafka Brokers，即Partition Replica分配至不同的Broker；</span><br><span class="line"></span><br><span class="line">#分配原则</span><br><span class="line">1、从Broker随机位置开始按照轮询方式选择每个Partition的第一个replica</span><br><span class="line">2、不同Partition剩余replica按照一定的偏移量紧跟着各自的第一个replica</span><br></pre></td></tr></table></div></figure>


        <h1 id="删除topic"   >
          <a href="#删除topic" class="heading-link"><i class="fas fa-link"></i></a><a href="#删除topic" class="headerlink" title="删除topic"></a>删除topic</h1>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh  --delete --topic tpc_1 --zookeeper node1：2181</span><br><span class="line"></span><br><span class="line">#（异步线程去删除）删除 topic,需要一个参数处于启用状态: delete.topic.enable = true,否则删不掉</span><br><span class="line"></span><br><span class="line">#使用 kafka-topics.sh 脚本删除主题的行为本质上只是在 ZooKeeper 中的 /admin/delete_topics 路径下 建一个与待删除主题同名的节点,以标记该主题为待删除的状态。与创建主题相同的是,真正删除主题的动作也是由 Kafka 的控制器负责完成的。</span><br></pre></td></tr></table></div></figure>


        <h1 id="查看topic"   >
          <a href="#查看topic" class="heading-link"><i class="fas fa-link"></i></a><a href="#查看topic" class="headerlink" title="查看topic"></a>查看topic</h1>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#(1)列出当前系统中的所有 topic </span><br><span class="line">bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --list</span><br><span class="line"></span><br><span class="line">#(2)查看 topic 详细信息</span><br><span class="line">bin/kafka-topics.sh --create --topic tpc_5-6   --zookeeper node1:2181 --replica-assignment 0:1,1:2</span><br><span class="line">bin/kafka-topics.sh --describe --topic tpc_5-6 --zookeeper node1:2181 </span><br></pre></td></tr></table></div></figure>


        <h1 id="增加分区数"   >
          <a href="#增加分区数" class="heading-link"><i class="fas fa-link"></i></a><a href="#增加分区数" class="headerlink" title="增加分区数"></a>增加分区数</h1>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --alter --topic tpc_1 --partitions 3 --zookeeper node1:2181</span><br></pre></td></tr></table></div></figure>


        <h1 id="动态配置topic-参数"   >
          <a href="#动态配置topic-参数" class="heading-link"><i class="fas fa-link"></i></a><a href="#动态配置topic-参数" class="headerlink" title="动态配置topic 参数"></a>动态配置topic 参数</h1>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#通过管理命令,可以为已创建的 topic 增加、修改、删除 topic level 参数</span><br><span class="line"></span><br><span class="line">#添加、修改配置参数(开启压缩发送传输种提高kafka消息吞吐量的有效办法(‘gzip’, ‘snappy’, ‘lz4’, ‘zstd’))</span><br><span class="line"></span><br><span class="line">bin/kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --add-config compression.type=gzip </span><br><span class="line"></span><br><span class="line">#删除配置参数</span><br><span class="line">bin/kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --delete-config compression.type</span><br></pre></td></tr></table></div></figure>


        <h1 id="生产消息到Kafka并进行消费"   >
          <a href="#生产消息到Kafka并进行消费" class="heading-link"><i class="fas fa-link"></i></a><a href="#生产消息到Kafka并进行消费" class="headerlink" title="生产消息到Kafka并进行消费"></a>生产消息到Kafka并进行消费</h1>
      <p>使用Kafka内置的测试程序，生产一些消息到Kafka的tpc_1主题中</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#example1-kafka-console-producer</span><br><span class="line">bin/kafka-console-producer.sh --broker-list node1:9092, node2:9092, node3:9092 --topic tpc_1</span><br><span class="line"></span><br><span class="line">&gt;hello word </span><br><span class="line">&gt;kafka </span><br><span class="line">&gt;nihao</span><br></pre></td></tr></table></div></figure>

<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#example2-kafka-console-consumer</span><br><span class="line">#(1)消费消息(从头开始)</span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server node1:9092, node2:9092, node1:9092 --topic tpc_1 --from-beginning</span><br><span class="line"></span><br><span class="line">#(2)指定要消费的分区,和要消费的起始 offset </span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server node1:9092,node2:9092,node3:9092 --topic tcp_1 --offset 2 --partition 0</span><br></pre></td></tr></table></div></figure>


        <h1 id="配置管理-kafka-configs"   >
          <a href="#配置管理-kafka-configs" class="heading-link"><i class="fas fa-link"></i></a><a href="#配置管理-kafka-configs" class="headerlink" title="配置管理 kafka-configs"></a>配置管理 kafka-configs</h1>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#示例:添加 topic 级别参数</span><br><span class="line">bin/kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type topics --entity-name tpc_2 --add-config cleanup.policy=compact , max.message.bytes=10000</span><br><span class="line"></span><br><span class="line">#使用 kafka-configs.sh 脚本来变更( alter )配置时,会在 ZooKeeper 中创建一个命名形式为: /config/&lt;entity-type&gt;/&lt;ent ity name &gt;的节点,并将变更的配置写入这个节点</span><br></pre></td></tr></table></div></figure>

</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/24/Sqoop%E9%85%8D%E7%BD%AE/">Sqoop配置</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-24</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-25</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="sqoop安装"   >
          <a href="#sqoop安装" class="heading-link"><i class="fas fa-link"></i></a><a href="#sqoop安装" class="headerlink" title="sqoop安装"></a>sqoop安装</h1>
      <p>安装sqoop的前提是已经具备java、mysql、hadoop和hive环境。</p>
<p>1、将sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz拖到node2中&#x2F;export&#x2F;software路径下</p>
<p>2、解压到&#x2F;export&#x2F;server路径下</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/software/</span><br><span class="line">tar -zxvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /expoort/server</span><br></pre></td></tr></table></div></figure>

<p>3、设置软链接</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/server/sqoop-1.4.6.bin__hadoop-2.0.4-alpha/ /export/server/sqoop</span><br></pre></td></tr></table></div></figure>

<p>4、修改环境变量</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line">#SQOOP_HOME</span><br><span class="line">export SQOOP_HOME=/export/server/sqoop</span><br><span class="line">export PATH=$PATH:$SQOOP_HOME/bin</span><br></pre></td></tr></table></div></figure>

<p>5、修改配置文件</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd $SQOOP_HOME/conf</span><br><span class="line">mv sqoop-env-template.sh sqoop-env.sh（sqoop-env-template.sh改名为sqoop-env.sh）</span><br><span class="line">vi sqoop-env.sh</span><br><span class="line">export HADOOP_COMMON_HOME= /export/server/hadoop</span><br><span class="line">export HADOOP_MAPRED_HOME= /export/server/hadoop</span><br><span class="line">export HIVE_HOME= /export/server/hive</span><br></pre></td></tr></table></div></figure>

<p>6、加入mysql的jdbc驱动包</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /export/server/hive/lib/mysql-connector-java-5.1.32.jar $SQOOP_HOME/lib/</span><br></pre></td></tr></table></div></figure>

<p>7、验证启动sqoop</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 本命令会列出所有mysql的数据库。</span><br><span class="line">cd /export/server/sqoop</span><br><span class="line">bin/sqoop list-databases \</span><br><span class="line"> --connect jdbc:mysql://node1:3306/ \</span><br><span class="line"> --username root --password Hadoop</span><br></pre></td></tr></table></div></figure>


        <h1 id="sqoop导入"   >
          <a href="#sqoop导入" class="heading-link"><i class="fas fa-link"></i></a><a href="#sqoop导入" class="headerlink" title="sqoop导入"></a>sqoop导入</h1>
      <p>“导入工具”导入单个表从RDBMS到HDFS。表中的每一行被视为HDFS的记录。所有记录都存储为文本文件的文本数据。</p>

        <h2 id="sqoop测试表数据"   >
          <a href="#sqoop测试表数据" class="heading-link"><i class="fas fa-link"></i></a><a href="#sqoop测试表数据" class="headerlink" title="sqoop测试表数据"></a>sqoop测试表数据</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在mysql中创建数据库userdb</span><br><span class="line">创建三张表: emp雇员表、emp_add雇员地址表、emp_conn雇员联系表</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/sqoop/1.png"></p>

        <h2 id="全量导入MySQL表数据到HDFS"   >
          <a href="#全量导入MySQL表数据到HDFS" class="heading-link"><i class="fas fa-link"></i></a><a href="#全量导入MySQL表数据到HDFS" class="headerlink" title="全量导入MySQL表数据到HDFS"></a>全量导入MySQL表数据到HDFS</h2>
      <p>该命令用于从MySQL数据库服务器中的emp表导入HDFS</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#example1-mysql-hdfs-start</span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--target-dir /sqoop/sqoopresult_test \</span><br><span class="line">--table emp --m 1</span><br></pre></td></tr></table></div></figure>

<p>为了验证在HDFS导入的数据，使用以下命令查看导入的数据</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -cat /sqoop/sqoopresult_test/part-m-00000</span><br></pre></td></tr></table></div></figure>

<p>在HDFS上默认用逗号,分隔emp表的数据和字段</p>
<p><img src="/../image/sqoop/2.png"></p>
<p>在web中查看</p>
<p><img src="/../image/sqoop/3.png"></p>

        <h2 id="在HDFS上用’-t’-分隔emp表的数据和字段"   >
          <a href="#在HDFS上用’-t’-分隔emp表的数据和字段" class="heading-link"><i class="fas fa-link"></i></a><a href="#在HDFS上用’-t’-分隔emp表的数据和字段" class="headerlink" title="在HDFS上用’\t’,分隔emp表的数据和字段"></a>在HDFS上用’\t’,分隔emp表的数据和字段</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#example2-mysql-hdfs-terminated</span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--target-dir /sqoop/sqoopresult_test2 \</span><br><span class="line">--fields-terminated-by &#x27;\t&#x27; \</span><br><span class="line">--table emp --m 1</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/sqoop/4.png"></p>

        <h2 id="使用–m-指定并行度"   >
          <a href="#使用–m-指定并行度" class="heading-link"><i class="fas fa-link"></i></a><a href="#使用–m-指定并行度" class="headerlink" title="使用–m 指定并行度"></a>使用–m 指定并行度</h2>
      <p>如果表的数据比较大可以并行启动多个maptask执行导入操作，使用–m 指定并行度</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#example3-mysql-hdfs-split</span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--target-dir /sqoop/sqoopresult_test3 \</span><br><span class="line">--fields-terminated-by &#x27;\t&#x27; \</span><br><span class="line">--split-by id \</span><br><span class="line">--table emp --m 2</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/sqoop/6.png"></p>
<p><img src="/../image/sqoop/7.png"></p>

        <h2 id="全量导入MySQL表数据到HIVE"   >
          <a href="#全量导入MySQL表数据到HIVE" class="heading-link"><i class="fas fa-link"></i></a><a href="#全量导入MySQL表数据到HIVE" class="headerlink" title="全量导入MySQL表数据到HIVE"></a>全量导入MySQL表数据到HIVE</h2>
      
        <h3 id="方式一：先复制表结构到hive中再导入数据"   >
          <a href="#方式一：先复制表结构到hive中再导入数据" class="heading-link"><i class="fas fa-link"></i></a><a href="#方式一：先复制表结构到hive中再导入数据" class="headerlink" title="方式一：先复制表结构到hive中再导入数据"></a>方式一：先复制表结构到hive中再导入数据</h3>
      <p>在hive中新建数据库sqoop_test用于测试</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#创建名为sqoop_test的数据库</span><br><span class="line">create database if not exists sqoop_test comment &quot;this is sqoop db&quot;;</span><br><span class="line">#切换到该数据库</span><br><span class="line">use sqoop_test;</span><br><span class="line">#查看该数据库中的表</span><br><span class="line">show tables;</span><br><span class="line">#查看该数据库中的表结构</span><br><span class="line">desc formatted emp_add_sp;</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/sqoop/8.png"></p>
<p>将关系型数据的表结构复制到hive中</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#example4-1-mysql-hive-structure</span><br><span class="line">bin/sqoop create-hive-table \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--table emp_add \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--hive-table sqoop_test.emp_add_sp</span><br></pre></td></tr></table></div></figure>

<p>从关系数据库导入文件到hive中</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#example4-2-mysql-hive-data</span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--table emp_add \</span><br><span class="line">--hive-table sqoop_test.emp_add_sp \</span><br><span class="line">--hive-import \</span><br><span class="line">--m 1</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/sqoop/9.png"></p>

        <h3 id="方式二：直接复制表结构数据到hive中"   >
          <a href="#方式二：直接复制表结构数据到hive中" class="heading-link"><i class="fas fa-link"></i></a><a href="#方式二：直接复制表结构数据到hive中" class="headerlink" title="方式二：直接复制表结构数据到hive中"></a>方式二：直接复制表结构数据到hive中</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#example5-mysql-hive</span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--table emp_conn \</span><br><span class="line">--hive-import \</span><br><span class="line">--m 1 \</span><br><span class="line">--hive-database sqoop_test;</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/sqoop/10.png"></p>

        <h2 id="导入表数据子集-where过滤"   >
          <a href="#导入表数据子集-where过滤" class="heading-link"><i class="fas fa-link"></i></a><a href="#导入表数据子集-where过滤" class="headerlink" title="导入表数据子集(where过滤)"></a>导入表数据子集(where过滤)</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#example6-mysql-hdfs-where</span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--where &quot;city =&#x27;sec-bad&#x27;&quot; \</span><br><span class="line">--target-dir /sqoop/wherequery_test \</span><br><span class="line">--table emp_add --m 1</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/sqoop/11.png"></p>

        <h2 id="导入表数据子集-query查询"   >
          <a href="#导入表数据子集-query查询" class="heading-link"><i class="fas fa-link"></i></a><a href="#导入表数据子集-query查询" class="headerlink" title="导入表数据子集(query查询)"></a>导入表数据子集(query查询)</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#example7-mysql-hdfs-query</span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--target-dir /sqoop/wherequery_test1 \</span><br><span class="line">--query &#x27;select id,name,deg from emp WHERE  id&gt;1203 and $CONDITIONS&#x27; \</span><br><span class="line">--split-by id \</span><br><span class="line">--fields-terminated-by &#x27;\001&#x27; \</span><br><span class="line">--m 2</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/sqoop/12.png"></p>
<p><img src="/../image/sqoop/13.png"></p>

        <h2 id="Append模式增量导入"   >
          <a href="#Append模式增量导入" class="heading-link"><i class="fas fa-link"></i></a><a href="#Append模式增量导入" class="headerlink" title="Append模式增量导入"></a>Append模式增量导入</h2>
      <p>执行以下指令先将我们之前的数据导入</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#example8-1-mysql-hdfs-append</span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--target-dir /sqoop/appendresult_test \</span><br><span class="line">--table emp --m 1</span><br></pre></td></tr></table></div></figure>

<p>使用hdfs dfs -cat查看生成的数据文件，发现数据已经导入到hdfs中</p>
<p><img src="/../image/sqoop/14.png"></p>
<p>在mysql的emp表中插入2条数据:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">insert into `userdb`.`emp` (`id`, `name`, `deg`, `salary`, `dept`) values (&#x27;1208&#x27;, &#x27;allenn&#x27;, &#x27;admin&#x27;, &#x27;30000&#x27;, &#x27;tp&#x27;);</span><br><span class="line">insert into `userdb`.`emp` (`id`, `name`, `deg`, `salary`, `dept`) values (&#x27;1209&#x27;, &#x27;woonn&#x27;, &#x27;admin&#x27;, &#x27;40000&#x27;, &#x27;tp&#x27;);</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/sqoop/15.png"></p>
<p>执行如下的指令，实现增量的导入:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#example8-2-mysql-hdfs-append</span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--table emp --m 1 \</span><br><span class="line">--target-dir /sqoop/appendresult_test \</span><br><span class="line">--incremental append \</span><br><span class="line">--check-column id \</span><br><span class="line">--last-value 1205</span><br></pre></td></tr></table></div></figure>

<p>最后验证导入数据目录 可以发现多了一个文件 里面就是增量数据</p>
<p><img src="/../image/sqoop/16.png"></p>

        <h2 id="Lastmodified模式增量导入"   >
          <a href="#Lastmodified模式增量导入" class="heading-link"><i class="fas fa-link"></i></a><a href="#Lastmodified模式增量导入" class="headerlink" title="Lastmodified模式增量导入"></a>Lastmodified模式增量导入</h2>
      <p>首先创建一个customer表，指定一个时间戳字段：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table customertest01(id int,name varchar(20),last_mod timestamp default current_timestamp on update current_timestamp);</span><br></pre></td></tr></table></div></figure>

<p>此处的时间戳设置为在数据的产生和更新时都会发生改变</p>
<p><img src="/../image/sqoop/17.png"></p>
<p>插入如下记录:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">insert into customertest01(id,name) values(1,&#x27;neil&#x27;);</span><br><span class="line">insert into customertest01(id,name) values(2,&#x27;jack&#x27;);</span><br><span class="line">insert into customertest01(id,name) values(3,&#x27;martin&#x27;);</span><br><span class="line">insert into customertest01(id,name) values(4,&#x27;tony&#x27;);</span><br><span class="line">insert into customertest01(id,name) values(5,&#x27;eric&#x27;);</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/sqoop/18.png"></p>
<p>此时执行sqoop指令将数据导入hdfs:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#example9-1-mysql-hdfs-Lastmodified</span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--target-dir /sqoop/lastmodifiedresult_test \</span><br><span class="line">--table customertest01 --m 1</span><br></pre></td></tr></table></div></figure>

<p>查看此时导入的结果数据：</p>
<p><img src="/../image/sqoop/19.png"></p>
<p>再次插入一条数据进入customertest表</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into customertest01(id,name) values(6,&#x27;james&#x27;)</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/sqoop/20.png"></p>

        <h2 id="使用incremental的方式进行增量的导入"   >
          <a href="#使用incremental的方式进行增量的导入" class="heading-link"><i class="fas fa-link"></i></a><a href="#使用incremental的方式进行增量的导入" class="headerlink" title="使用incremental的方式进行增量的导入:"></a>使用incremental的方式进行增量的导入:</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--table customertest01 \</span><br><span class="line">--target-dir /sqoop/lastmodifiedresult_test \</span><br><span class="line">--check-column last_mod \</span><br><span class="line">--incremental lastmodified \</span><br><span class="line">--last-value &quot;2023-06-03 18:42:06&quot; \</span><br><span class="line">--m 1 \</span><br><span class="line">--append</span><br></pre></td></tr></table></div></figure>

<p>查看此时导入的结果数据:</p>
<p><img src="/../image/sqoop/21.png"></p>

        <h2 id="Lastmodified模式-merge-key-合并-模式添加"   >
          <a href="#Lastmodified模式-merge-key-合并-模式添加" class="heading-link"><i class="fas fa-link"></i></a><a href="#Lastmodified模式-merge-key-合并-模式添加" class="headerlink" title="Lastmodified模式:merge-key(合并)模式添加"></a>Lastmodified模式:merge-key(合并)模式添加</h2>
      <p>1、去更新 customertest01表中id为1的name字段</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">update customertest01 set name = &#x27;Neil&#x27; where id = 1;</span><br></pre></td></tr></table></div></figure>

<p>更新之后，这条数据的时间戳会更新为更新数据时的系统时间</p>
<p>2、执行如下指令，把id字段作为merge-key:</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#example10-mysql-hdfs-merge-key</span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--table customertest01 \</span><br><span class="line">--target-dir /sqoop/lastmodifiedresult_test \</span><br><span class="line">--check-column last_mod \</span><br><span class="line">--incremental lastmodified \</span><br><span class="line">--last-value &quot;2013-06-3 18:42:06&quot; \</span><br><span class="line">--m 1 \</span><br><span class="line">--merge-key id</span><br></pre></td></tr></table></div></figure>

<p>由于merge-key这种模式是进行了一次完整的mapreduce操作，因此最终我们在lastmodifiedresult_test文件夹下可以看到生成的为part-r-00000这样的文件，会发现id&#x3D;1的name已经得到修改，同时新增了id&#x3D;6的数据。</p>
<p><img src="/../image/sqoop/22.png"></p>

        <h1 id="sqoop导出"   >
          <a href="#sqoop导出" class="heading-link"><i class="fas fa-link"></i></a><a href="#sqoop导出" class="headerlink" title="sqoop导出"></a>sqoop导出</h1>
      <p>将数据从Hadoop生态体系导出到RDBMS数据库导出前，目标表必须存在于目标数据库中。</p>

        <h2 id="默认模式导出HDFS数据到mysql"   >
          <a href="#默认模式导出HDFS数据到mysql" class="heading-link"><i class="fas fa-link"></i></a><a href="#默认模式导出HDFS数据到mysql" class="headerlink" title="默认模式导出HDFS数据到mysql"></a>默认模式导出HDFS数据到mysql</h2>
      <p>1、准备HDFS数据</p>
<p>在HDFS文件系统中“&#x2F;emp&#x2F;”目录的下创建一个文件emp_data.txt：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /export/data/sqoop-data/emp/</span><br><span class="line">vim emp_data.txt</span><br><span class="line">1201,gopal,manager,50000,TP</span><br><span class="line">1202,manisha,preader,50000,TP</span><br><span class="line">1203,kalil,php dev,30000,AC</span><br><span class="line">1204,prasanth,php dev,30000,AC</span><br><span class="line">1205,kranthi,admin,20000,TP</span><br><span class="line">1206,satishp,grpdes,20000,GR</span><br><span class="line">#上传至hdfs</span><br><span class="line">hadoop fs -mkdir /sqoop/emp_data</span><br><span class="line">hadoop fs -put emp_data.txt /sqoop/emp_data</span><br></pre></td></tr></table></div></figure>

<p>2、手动创建mysql中的目标表</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; use userdb;</span><br><span class="line"></span><br><span class="line">mysql&gt; create table employee ( </span><br><span class="line"></span><br><span class="line">  id int not null primary key, </span><br><span class="line"></span><br><span class="line">  name varchar(20), </span><br><span class="line"></span><br><span class="line">  deg varchar(20),</span><br><span class="line"></span><br><span class="line">  salary int,</span><br><span class="line"></span><br><span class="line">  dept varchar(10));</span><br></pre></td></tr></table></div></figure>

<p>3、执行导出命令</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#example10-hdfs-mysql-export</span><br><span class="line"></span><br><span class="line">bin/sqoop export \</span><br><span class="line"></span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line"></span><br><span class="line">--username root \</span><br><span class="line"></span><br><span class="line">--password hadoop \</span><br><span class="line"></span><br><span class="line">--table employee \</span><br><span class="line"></span><br><span class="line">--columns id,name,deg,salary,dept \</span><br><span class="line"></span><br><span class="line">--export-dir /sqoop/emp_data/</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/sqoop/5.png"></p>

        <h2 id="更新导出（updateonly模式）"   >
          <a href="#更新导出（updateonly模式）" class="heading-link"><i class="fas fa-link"></i></a><a href="#更新导出（updateonly模式）" class="headerlink" title="更新导出（updateonly模式）"></a>更新导出（updateonly模式）</h2>
      <p>1、准备HDFS数据</p>
<p>在HDFS文件系统中&#x2F;sqoop&#x2F;updateonly_1&#x2F;目录的下创建一个文件updateonly_1.txt：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /export/data/sqoop-data/updateonly_1</span><br><span class="line">vim updateonly_1.txt</span><br><span class="line">1201,gopal,manager,50000</span><br><span class="line">1202,manisha,preader,50000</span><br><span class="line">1203,kalil,php dev,30000</span><br><span class="line">\#上传至hdfs</span><br><span class="line">hadoop fs -mkdir /sqoop/emp_data</span><br><span class="line">hadoop fs -put updateonly_1.txt /sqoop/updateonly_1</span><br></pre></td></tr></table></div></figure>

<p>2、手动创建mysql中的目标表</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; USE userdb;</span><br><span class="line">mysql&gt; CREATE TABLE updateonly ( </span><br><span class="line">  id INT NOT NULL PRIMARY KEY, </span><br><span class="line">  name VARCHAR(20), </span><br><span class="line">  deg VARCHAR(20),</span><br><span class="line">  salary INT);</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/sqoop/23.png"></p>
<p>3、执行全部导出操作</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#example11-1-hdfs-mysql-export-updateonly</span><br><span class="line">bin/sqoop export \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--table updateonly \</span><br><span class="line">--export-dir /sqoop/updateonly_1/</span><br></pre></td></tr></table></div></figure>

<p>4、查看此时mysql中的数据</p>
<p>全量导出</p>
<p><img src="/../image/sqoop/24.png"></p>
<p>5、新增一个文件</p>
<p>新增一个文件updateonly_2.txt：修改了前三条数据并且新增了一条记录</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1201,gopal,manager,1212</span><br><span class="line">1202,manisha,preader,1313</span><br><span class="line">1203,kalil,php dev,1414</span><br><span class="line">1204,allen,java,1515</span><br><span class="line">hadoop fs -mkdir /sqoop/updateonly_2</span><br><span class="line">hadoop fs -put updateonly_2.txt /sqoop/updateonly_2</span><br></pre></td></tr></table></div></figure>

<p>6、执行更新导出</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#example11-2-hdfs-mysql-export-updateonly</span><br><span class="line">bin/sqoop export \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--table updateonly \</span><br><span class="line">--export-dir /sqoop/updateonly_2 \</span><br><span class="line">--update-key id \</span><br><span class="line">--update-mode updateonly</span><br></pre></td></tr></table></div></figure>

<p>7、查看最终结果</p>
<p><img src="/../image/sqoop/24.png"></p>

        <h2 id="更新导出（allowinsert模式）"   >
          <a href="#更新导出（allowinsert模式）" class="heading-link"><i class="fas fa-link"></i></a><a href="#更新导出（allowinsert模式）" class="headerlink" title="更新导出（allowinsert模式）"></a>更新导出（allowinsert模式）</h2>
      <p>1、在HDFS &#x2F;sqoop&#x2F;allowinsert_1&#x2F;目录的下创建一个文件allowinsert_1.txt：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /export/data/sqoop-data/allowinsert_1/</span><br><span class="line">cd /export/data/sqoop-data/allowinsert_1/</span><br><span class="line">vim allowinsert_1.txt</span><br><span class="line">1201,gopal,manager,50000</span><br><span class="line">1202,manisha,preader,50000</span><br><span class="line">1203,kalil,php dev,30000</span><br><span class="line">#上传至hdfs</span><br><span class="line">hadoop fs -mkdir /sqoop/allowinsert_1</span><br><span class="line">hadoop fs -put allowinsert_1.txt /sqoop/allowinsert_1</span><br></pre></td></tr></table></div></figure>

<p>2、手动创建mysql中的目标表</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; USE userdb;</span><br><span class="line">mysql&gt; CREATE TABLE allowinsert ( </span><br><span class="line">   id INT NOT NULL PRIMARY KEY, </span><br><span class="line">   name VARCHAR(20), </span><br><span class="line">   deg VARCHAR(20),</span><br><span class="line">   salary INT);</span><br></pre></td></tr></table></div></figure>

<p>3、先执行全部导出操作</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#example12-1-hdfs-mysql-export-allowinsert</span><br><span class="line">bin/sqoop export \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--table allowinsert \</span><br><span class="line">--export-dir /sqoop/allowinsert_1/</span><br></pre></td></tr></table></div></figure>

<p>4、查看此时mysql中的数据</p>
<p><img src="/../image/sqoop/26.png"></p>
<p>5、新增文件</p>
<p>创建文件allowinsert_2.txt。修改前三条数据并且新增了一条记录。上传至 &#x2F;sqoop&#x2F;allowinsert_2&#x2F;目录下：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /export/data/sqoop-data/allowinsert_2/</span><br><span class="line">cd /export/data/sqoop-data/allowinsert_2/</span><br><span class="line">vim allowinsert_2.txt</span><br><span class="line">1201,gopal,manager,1212</span><br><span class="line">1202,manisha,preader,1313</span><br><span class="line">1203,kalil,php dev,1414</span><br><span class="line">1204,allen,java,1515</span><br><span class="line">#上传至hdfs</span><br><span class="line">hadoop fs -mkdir /sqoop/allowinsert_2</span><br><span class="line">hadoop fs -put allowinsert_2.txt /sqoop/allowinsert_2</span><br></pre></td></tr></table></div></figure>

<p>6、执行更新导出</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#example12-2-hdfs-mysql-export-allowinsert</span><br><span class="line">bin/sqoop export \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root --password hadoop \</span><br><span class="line">--table allowinsert \</span><br><span class="line">--export-dir /sqoop/allowinsert_2/ \</span><br><span class="line">--update-key id \</span><br><span class="line">--update-mode allowinsert</span><br></pre></td></tr></table></div></figure>

<p>7、查看最终结果</p>
<p><img src="/../image/sqoop/27.png"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/24/Flume/">Flume</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-24</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-25</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="安装flume"   >
          <a href="#安装flume" class="heading-link"><i class="fas fa-link"></i></a><a href="#安装flume" class="headerlink" title="安装flume"></a>安装flume</h1>
      <p>Flume下载地址：<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="http://flume.apache.org/download.html" >http://flume.apache.org/download.html</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h1 id="解压"   >
          <a href="#解压" class="heading-link"><i class="fas fa-link"></i></a><a href="#解压" class="headerlink" title="解压"></a>解压</h1>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/flume/</span><br><span class="line">ls</span><br><span class="line"># 将 apache-flume-1.9.0-bin.tar.gz下载到CentOS系统中，对其解压</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/flume/1.png"></p>

        <h1 id="添加软连接"   >
          <a href="#添加软连接" class="heading-link"><i class="fas fa-link"></i></a><a href="#添加软连接" class="headerlink" title="添加软连接"></a>添加软连接</h1>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s apache-flume-1.9.0-bin flume</span><br></pre></td></tr></table></div></figure>


        <h1 id="配置环境变量"   >
          <a href="#配置环境变量" class="heading-link"><i class="fas fa-link"></i></a><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h1>
      <p>在&#x2F;etc&#x2F;profile文件中添加以下语句</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#FLUME_HOME</span><br><span class="line">export FLUME_HOME=/export/server/flume</span><br><span class="line">export PATH=$PATH:$FLUME_HOME/bin</span><br></pre></td></tr></table></div></figure>


        <h1 id="查看是否设置成功"   >
          <a href="#查看是否设置成功" class="heading-link"><i class="fas fa-link"></i></a><a href="#查看是否设置成功" class="headerlink" title="查看是否设置成功"></a>查看是否设置成功</h1>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo $FLUME_HOME</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/flume/2.png"></p>

        <h1 id="安装netcat"   >
          <a href="#安装netcat" class="heading-link"><i class="fas fa-link"></i></a><a href="#安装netcat" class="headerlink" title="安装netcat"></a>安装netcat</h1>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y nc</span><br></pre></td></tr></table></div></figure>


        <h1 id="添加配置文件"   >
          <a href="#添加配置文件" class="heading-link"><i class="fas fa-link"></i></a><a href="#添加配置文件" class="headerlink" title="添加配置文件"></a>添加配置文件</h1>
      <p>在netcat-logger.conf中添加以下语句</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># example1-netcat-logger.conf: 单节点Flume配置</span><br><span class="line"># 定义agent名称为a1</span><br><span class="line"># 设置3个组件的名称</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"># 配置source类型为NetCat,监听地址为本机，端口为44444</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = 0.0.0.0</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line">#source和channel关联</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"># 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"># 配置sink类型为Logger</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"># 将sink绑定到channel上</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">启动agent</span><br><span class="line">bin/flume-ng agent -n a1 -c conf -f myconf/example1-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></div></figure>


        <h1 id="使用Netcat测试"   >
          <a href="#使用Netcat测试" class="heading-link"><i class="fas fa-link"></i></a><a href="#使用Netcat测试" class="headerlink" title="使用Netcat测试"></a>使用Netcat测试</h1>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc node1 44444</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/flume/3.png"></p>

        <h1 id="exec-source测试"   >
          <a href="#exec-source测试" class="heading-link"><i class="fas fa-link"></i></a><a href="#exec-source测试" class="headerlink" title="exec_source测试"></a>exec_source测试</h1>
      <p>在example2-exec-source-logger.conf添加以下语句</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># example2-exec-source-logger.conf</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sources.r1.command = tail -F /export/data/flume-example-data/shell/access.log </span><br><span class="line">a1.sources.r1.batchSize = 100</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></div></figure>

<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 启动测试</span><br><span class="line">bin/flume-ng agent -n a1 -c conf/ -f myconf/example2-exec-source-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/flume/4.png"></p>

        <h1 id="spooldir-source测试"   >
          <a href="#spooldir-source测试" class="heading-link"><i class="fas fa-link"></i></a><a href="#spooldir-source测试" class="headerlink" title="spooldir_source测试"></a>spooldir_source测试</h1>
      <p>配置文件</p>
<p>在example3-spooldir-source.conf文件中添加以下语句</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># example3-spooldir-source.conf</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sources.r1.type = spooldir</span><br><span class="line">a1.sources.r1.spoolDir = /export/data/flume-example-data/weblog </span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></div></figure>

<p>启动测试</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c conf -f myconf/example3-spooldir-source.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/flume/5.png"></p>

        <h1 id="taildir-source测试"   >
          <a href="#taildir-source测试" class="heading-link"><i class="fas fa-link"></i></a><a href="#taildir-source测试" class="headerlink" title="taildir_source测试"></a>taildir_source测试</h1>
      <p>配置文件</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#example4-taildir-source.conf</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sources.r1.positionFile = /export/data/flume-example-data/flumedata/taildir_position.json</span><br><span class="line">a1.sources.r1.filegroups = g1 g2</span><br><span class="line">a1.sources.r1.filegroups.g1 = /export/data/flume-example-data/weblog/web.*</span><br><span class="line">a1.sources.r1.filegroups.g2 = /export/data/flume-example-data/wxlog/wx.*</span><br><span class="line">a1.sources.r1.fileHeader = true</span><br><span class="line">#动态的header-keys eg：filepath=/../../../</span><br><span class="line">a1.sources.r1.fileHeaderKey = filepath</span><br><span class="line"></span><br><span class="line">#写死的header-keys（静态的） eg:a1 = aa1</span><br><span class="line">a1.sources.r1.headers.g1.a1 = aa1</span><br><span class="line">a1.sources.r1.headers.g1.b1 = bb1</span><br><span class="line">a1.sources.r1.headers.g2.a2 = aa2</span><br><span class="line">a1.sources.r1.headers.g2.b2 = bb2</span><br><span class="line"></span><br><span class="line">a1.sources.r1.maxBatchCount = 1000</span><br><span class="line"></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 10000</span><br><span class="line">a1.channels.c1.transactionCapacity = 1000</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></div></figure>

<p>启动测试</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c conf/ -f  myconf/example4-taildir-source.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/flume/6.png"></p>

        <h1 id="avro-source"   >
          <a href="#avro-source" class="heading-link"><i class="fas fa-link"></i></a><a href="#avro-source" class="headerlink" title="avro source"></a>avro source</h1>
      <p>配置文件</p>
<p>在example5-avro-source.conf文件中添加以下语句</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#example5-avro-source.conf</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sources.r1.bind = 0.0.0.0</span><br><span class="line">a1.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 200</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></div></figure>

<p>启动测试</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 启动agent</span><br><span class="line">bin/flume-ng agent -c conf -f  myconf/example5-avro-source.conf -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/flume/7.png"></p>

        <h1 id="利用avro-source-和avro-sink实现agent级联"   >
          <a href="#利用avro-source-和avro-sink实现agent级联" class="heading-link"><i class="fas fa-link"></i></a><a href="#利用avro-source-和avro-sink实现agent级联" class="headerlink" title="利用avro source 和avro sink实现agent级联"></a>利用avro source 和avro sink实现agent级联</h1>
      <p>上游服务器配置 example7-1-taildir-f-avro.conf</p>
<p><img src="/../image/flume/8.png"></p>
<p>下游服务器配置 example7-2-avro-f-hdfs.conf</p>
<p><img src="/../image/flume/9.png"></p>
<p>根据级联操作手册生成相应数据</p>
<p><img src="/../image/flume/10.png"></p>

        <h1 id="拦截器"   >
          <a href="#拦截器" class="heading-link"><i class="fas fa-link"></i></a><a href="#拦截器" class="headerlink" title="拦截器"></a>拦截器</h1>
      <p>配置</p>
<p>在myconf中添加example8-interceptor.conf</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># example8-interceptor.conf</span><br><span class="line"># 定义agent名称为a1</span><br><span class="line"># 设置3个组件的名称</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># 配置source类型为NetCat,监听地址为本机，端口为44444</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = 0.0.0.0</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"># 配置拦截器为host</span><br><span class="line">a1.sources.r1.interceptors = i1 </span><br><span class="line">a1.sources.r1.interceptors.i1.type = host</span><br><span class="line"></span><br><span class="line"># 配置sink类型为Logger</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># 将source和sink绑定到channel上</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">启动flume</span><br><span class="line">bin/flume-ng agent -n a1 -c conf -f myconf/example8-interceptor.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/flume/11.png"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/24/Kafka%E9%85%8D%E7%BD%AE/">Kafka配置</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-24</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-25</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="安装kafka-集群"   >
          <a href="#安装kafka-集群" class="heading-link"><i class="fas fa-link"></i></a><a href="#安装kafka-集群" class="headerlink" title="安装kafka 集群"></a>安装kafka 集群</h1>
      <p>上传压缩包到&#x2F;export&#x2F;server目录下</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv kafka_2.12-2.4.1.tgz /export/server</span><br></pre></td></tr></table></div></figure>

<p>解压</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf kafka_2.12-2.4.1.tgz</span><br></pre></td></tr></table></div></figure>

<p>进入配置文件目录</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/kafka_2.11-2.0.0/config</span><br></pre></td></tr></table></div></figure>

<p>编辑配置文件</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vi server.properties</span><br><span class="line">加入一下内容；</span><br><span class="line">#为依次增长的:0、1、2、3、4,集群中唯一 id --》从0开始，每台不能重复，第一块要改的</span><br><span class="line">broker.id=0 </span><br><span class="line"></span><br><span class="line">----Logbasic------</span><br><span class="line">#数据存储的目录，第二块要改的</span><br><span class="line">log.dirs=/export/data/kafka-logs  </span><br><span class="line"></span><br><span class="line">---zookeeper----</span><br><span class="line">#指定 zk 集群地址，第四块要改的</span><br><span class="line">zookeeper.connect=node1:2181,node2:2181,node3:2181</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/kfk/1.png"></p>
<p>将node1内容分发到node2，node3中</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/kafka_2.12-2.4.1 root@node2: /export/server</span><br><span class="line">scp -r /export/server/kafka_2.12-2.4.1 root@node3: /export/server</span><br></pre></td></tr></table></div></figure>

<p>配置环境变量</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">进入profile；</span><br><span class="line">vi /etc/profile </span><br><span class="line">添加一下内容；</span><br><span class="line">export KAFKA_HOME=/export/server/kafka </span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin</span><br></pre></td></tr></table></div></figure>

<p>添加后重置环境</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></div></figure>

<p>分发环境变量至node2，node3</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp /etc/profile root@node2: /etc/profile</span><br><span class="line">scp /etc/profile root@node3: /etc/profile</span><br></pre></td></tr></table></div></figure>

<p>分别在node2和node3上修改配置文件</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">broker.id=1 </span><br><span class="line">broker.id=2</span><br></pre></td></tr></table></div></figure>


        <h1 id="启停集群-在各个节点上启动"   >
          <a href="#启停集群-在各个节点上启动" class="heading-link"><i class="fas fa-link"></i></a><a href="#启停集群-在各个节点上启动" class="headerlink" title="启停集群(在各个节点上启动)"></a>启停集群(在各个节点上启动)</h1>
      <p>启动集群</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-start.sh -daemon /export/server/kafka/config/server.properties </span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/kfk/2.png"></p>
<p>停止集群</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-stop.sh stop</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/kfk/3.png"></p>

        <h1 id="kafka一键启停脚本"   >
          <a href="#kafka一键启停脚本" class="heading-link"><i class="fas fa-link"></i></a><a href="#kafka一键启停脚本" class="headerlink" title="kafka一键启停脚本"></a>kafka一键启停脚本</h1>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">if [ $# -eq 0 ]</span><br><span class="line">then</span><br><span class="line">echo &quot;please input param:start stop&quot;</span><br><span class="line">else</span><br><span class="line"></span><br><span class="line">if [ $1 = start  ]</span><br><span class="line">then</span><br><span class="line">for i in &#123;1..3&#125;</span><br><span class="line">do</span><br><span class="line">echo &quot;$&#123;1&#125;ing node$&#123;i&#125;&quot;</span><br><span class="line">ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/kafka/bin/kafka-server-start.sh -daemon /export/server/kafka/config/server.properties&quot;</span><br><span class="line">done</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $1 = stop ]</span><br><span class="line">then</span><br><span class="line">for i in &#123;1..3&#125;</span><br><span class="line">do</span><br><span class="line">ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/kafka/bin/kafka-server-stop.sh&quot;</span><br><span class="line">done</span><br><span class="line">fi</span><br><span class="line">fi</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/kfk/4.png"></p>

        <h1 id="kafka命令行操作"   >
          <a href="#kafka命令行操作" class="heading-link"><i class="fas fa-link"></i></a><a href="#kafka命令行操作" class="headerlink" title="kafka命令行操作"></a>kafka命令行操作</h1>
      
        <h2 id="创建topic"   >
          <a href="#创建topic" class="heading-link"><i class="fas fa-link"></i></a><a href="#创建topic" class="headerlink" title="创建topic"></a>创建topic</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#基本方式</span><br><span class="line">./kafka-topics.sh --create --topic tpc_1 --partitions 2 --replication-factor 2 --zookeeper node1:2181</span><br><span class="line">bootstrap方式</span><br><span class="line">创建名为test的主题</span><br><span class="line">bin/kafka-topics.sh --create --bootstrap-server node1:9092 --topic test</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/kfk/5.png"></p>

        <h2 id="查看目前Kafka中的主题"   >
          <a href="#查看目前Kafka中的主题" class="heading-link"><i class="fas fa-link"></i></a><a href="#查看目前Kafka中的主题" class="headerlink" title="查看目前Kafka中的主题"></a>查看目前Kafka中的主题</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --list --bootstrap-server node1:9092</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/kfk/6.png"></p>

        <h2 id="删除topic"   >
          <a href="#删除topic" class="heading-link"><i class="fas fa-link"></i></a><a href="#删除topic" class="headerlink" title="删除topic"></a>删除topic</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh  --delete --topic tpc_1 --zookeeper node1：2181</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/kfk/7.png"></p>

        <h2 id="列出当前系统中的所有-topic"   >
          <a href="#列出当前系统中的所有-topic" class="heading-link"><i class="fas fa-link"></i></a><a href="#列出当前系统中的所有-topic" class="headerlink" title="列出当前系统中的所有 topic"></a>列出当前系统中的所有 topic</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --list</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/kfk/8.png"></p>

        <h2 id="查看-topic-详细信息"   >
          <a href="#查看-topic-详细信息" class="heading-link"><i class="fas fa-link"></i></a><a href="#查看-topic-详细信息" class="headerlink" title="查看 topic 详细信息"></a>查看 topic 详细信息</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --topic tpc_1   --zookeeper node1:2181 --replica-assignment 0:1,1:2</span><br><span class="line">bin/kafka-topics.sh --describe --topic tpc_1 --zookeper node1:2181</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/kfk/9.png"></p>

        <h2 id="增加分区数"   >
          <a href="#增加分区数" class="heading-link"><i class="fas fa-link"></i></a><a href="#增加分区数" class="headerlink" title="增加分区数"></a>增加分区数</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --alter --topic tpc_1 --partitions 3 --zookeeper node1:2181</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/kfk/10.png"></p>

        <h2 id="消费消息-从头开始"   >
          <a href="#消费消息-从头开始" class="heading-link"><i class="fas fa-link"></i></a><a href="#消费消息-从头开始" class="headerlink" title="消费消息(从头开始)"></a>消费消息(从头开始)</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server node1:9092, node2:9092, node1:9092 --topic tpc_1 --from-beginning</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/kfk/11.png"></p>

        <h2 id="指定要消费的分区-和要消费的起始-offset"   >
          <a href="#指定要消费的分区-和要消费的起始-offset" class="heading-link"><i class="fas fa-link"></i></a><a href="#指定要消费的分区-和要消费的起始-offset" class="headerlink" title="指定要消费的分区,和要消费的起始 offset"></a>指定要消费的分区,和要消费的起始 offset</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server node1:9092,node2:9092,node3:9092 --topic tcp_1 --offset 2 --partition 0</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/kfk/12.png"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/24/zookeeper%E9%83%A8%E7%BD%B2/">zookeeper部署</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-24</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-25</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="准备工作"   >
          <a href="#准备工作" class="heading-link"><i class="fas fa-link"></i></a><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1>
      <p>安装前需要安装好jdk</p>
<p>检测集群时间是否同步</p>
<p>检测防火墙是否关闭</p>
<p>检测主机 ip映射有没有配置</p>
<p><img src="/../image/zookeeper/1.png"></p>

        <h1 id="解压以及设置软连接"   >
          <a href="#解压以及设置软连接" class="heading-link"><i class="fas fa-link"></i></a><a href="#解压以及设置软连接" class="headerlink" title="解压以及设置软连接"></a>解压以及设置软连接</h1>
      <p>在node1主机上，解压zookeeper的压缩包到&#x2F;export&#x2F;server路径下去，然后准备进行安装</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /export/software</span><br><span class="line">tar -zxvf zookeeper.tar.gz -C /export/server/</span><br><span class="line">cd /export/server/</span><br><span class="line">ln -s zookeeper/ zookeeper</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/zookeeper/2.png"></p>

        <h1 id="环境变量"   >
          <a href="#环境变量" class="heading-link"><i class="fas fa-link"></i></a><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h1>
      <p>修改环境变量（注意：3台zookeeper都需要修改）</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line">export ZOOKEEPER_HOME=/export/server/zookeeper</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></div></figure>


        <h1 id="配置文件"   >
          <a href="#配置文件" class="heading-link"><i class="fas fa-link"></i></a><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h1>
      <p>修改Zookeeper配置文件</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/zookeeper/conf/</span><br><span class="line">cp zoo_sample.cfg zoo.cfg</span><br><span class="line">mkdir -p /export/data/zookeeper/zkdatas/</span><br><span class="line">vim zoo.cfg</span><br></pre></td></tr></table></div></figure>


        <h1 id="添加myid配置"   >
          <a href="#添加myid配置" class="heading-link"><i class="fas fa-link"></i></a><a href="#添加myid配置" class="headerlink" title="添加myid配置"></a>添加myid配置</h1>
      <p>在node1主机的&#x2F;export&#x2F;data&#x2F;zookeeper&#x2F;zkdatas这个路径下创建一个文件，文件名为myid ,文件内容为1</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt; /export/data/zkdatas/myid</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/zookeeper/3.png"></p>

        <h1 id="安装包分发并修改myid的值"   >
          <a href="#安装包分发并修改myid的值" class="heading-link"><i class="fas fa-link"></i></a><a href="#安装包分发并修改myid的值" class="headerlink" title="安装包分发并修改myid的值"></a>安装包分发并修改myid的值</h1>
      <p>在node1主机上，将安装包分发到其他机器</p>
<p>第一台机器上面执行以下两个命令</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/</span><br><span class="line">scp -r /export/server/zookeeper-3.4.6/ root@node2:/export/server/</span><br><span class="line">scp -r /export/server/zookeeper-3.4.6/ root@node2:/export/server/</span><br></pre></td></tr></table></div></figure>

<p>第二台机器上建立软连接, 并修改myid的值为2</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/</span><br><span class="line">ln -s zookeeper-3.4.6/ zookeeper</span><br><span class="line">echo 2 &gt; /export/data/zookeeper/zkdatas/myid</span><br></pre></td></tr></table></div></figure>

<p>第三台机器上建立软连接, 并修改myid的值为3</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/</span><br><span class="line">ln -s zookeeper-3.4.6/ zookeeper</span><br><span class="line">echo 3 &gt; /export/data/zookeeper/zkdatas/myid</span><br></pre></td></tr></table></div></figure>


        <h1 id="三台机器启动zookeeper服务"   >
          <a href="#三台机器启动zookeeper服务" class="heading-link"><i class="fas fa-link"></i></a><a href="#三台机器启动zookeeper服务" class="headerlink" title="三台机器启动zookeeper服务"></a>三台机器启动zookeeper服务</h1>
      <p>三台机器分别启动zookeeper服务</p>
<p>这个命令三台机器都要执行</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/export/server/zookeeper/bin/zkServer.sh start</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/zookeeper/4.png"></p>
<p><img src="/../image/zookeeper/5.png"></p>
<p><img src="/../image/zookeeper/6.png"></p>
<p>三台主机分别查看启动状态</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/export/server/zookeeper/bin/zkServer.sh status</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/zookeeper/7.png"></p>
<p><img src="/../image/zookeeper/8.png"></p>
<p><img src="/../image/zookeeper/9.png"></p>

        <h1 id="启动（每台机器）"   >
          <a href="#启动（每台机器）" class="heading-link"><i class="fas fa-link"></i></a><a href="#启动（每台机器）" class="headerlink" title="启动（每台机器）"></a>启动（每台机器）</h1>
      <p>zkServer.sh start</p>
<p>编写一个脚本来批量启动所有机器</p>
<p>1.创建&#x2F;export&#x2F;server&#x2F;start&#x2F;zk_start目录</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /export/shell</span><br></pre></td></tr></table></div></figure>

<p>2.编辑创建zkall.sh</p>
<p><img src="/../image/zookeeper/10.png"></p>
<p>3.写shell脚本</p>
<p><img src="/../image/zookeeper/11.png"></p>
<p>4.配置zk脚本环境变量</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#ZOOKEEPER_SHELL_HOME</span><br><span class="line">export ZKS_HOME=/export/shell/</span><br><span class="line">export PATH=$PATH:$ZKS_HOME</span><br></pre></td></tr></table></div></figure>

<p>5.zookeeper的环境变量</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export ZK_HOME=/export/server/zookeeper</span><br><span class="line">export PATH=$&#123;ZK_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></div></figure>

<p>6.让环境变量生效</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></div></figure>

<p>7.启动测试</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod 777 /export/shell/zkall.sh</span><br><span class="line">zkall.sh start</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/zookeeper/12.png"></p>
<p>启动成功，测试结束</p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/21/Kafka%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">Kafka环境配置</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-21</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-25</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="一、安装zookeeper"   >
          <a href="#一、安装zookeeper" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、安装zookeeper" class="headerlink" title="一、安装zookeeper"></a>一、安装zookeeper</h1>
      <p>Kafka集群是必须要有ZooKeeper</p>

        <h1 id="二、安装kadka集群"   >
          <a href="#二、安装kadka集群" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、安装kadka集群" class="headerlink" title="二、安装kadka集群"></a>二、安装kadka集群</h1>
      
        <h2 id="1、上传安装title-Kafka环境配置包"   >
          <a href="#1、上传安装title-Kafka环境配置包" class="heading-link"><i class="fas fa-link"></i></a><a href="#1、上传安装title-Kafka环境配置包" class="headerlink" title="1、上传安装title: Kafka环境配置包"></a>1、上传安装title: Kafka环境配置包</h2>
      <p>上传kafka_2.12-2.4.1.tgz到&#x2F;export&#x2F;server目录下，解压</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server</span><br><span class="line">tar -zxvf kafka_2.12-2.4.1.tgz</span><br></pre></td></tr></table></div></figure>


        <h2 id="2、修改配置文件"   >
          <a href="#2、修改配置文件" class="heading-link"><i class="fas fa-link"></i></a><a href="#2、修改配置文件" class="headerlink" title="2、修改配置文件"></a>2、修改配置文件</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 进入配置文件目录</span><br><span class="line">cd /export/server/kafka_2.11-2.0.0/config</span><br><span class="line"># 编辑配置文件</span><br><span class="line">vi server.properties</span><br><span class="line"></span><br><span class="line"># 为依次增长的:0、1、2、3、4,集群中唯一 id从0开始，每台不能重复，第一块要改的</span><br><span class="line">broker.id=0 </span><br><span class="line"></span><br><span class="line">----Logbasic------</span><br><span class="line">#数据存储的目录，第二块要改的</span><br><span class="line">log.dirs=/export/data/kafka-logs  </span><br><span class="line"></span><br><span class="line">---zookeeper----</span><br><span class="line">#指定 zk 集群地址，第四块要改的</span><br><span class="line">zookeeper.connect=node1:2181,node2:2181,node3:2181</span><br></pre></td></tr></table></div></figure>


        <h2 id="3、分发kafka"   >
          <a href="#3、分发kafka" class="heading-link"><i class="fas fa-link"></i></a><a href="#3、分发kafka" class="headerlink" title="3、分发kafka"></a>3、分发kafka</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/</span><br><span class="line">syncfile /export/server/kafka_2.12-2.4.1</span><br></pre></td></tr></table></div></figure>


        <h2 id="4、配置环境变量"   >
          <a href="#4、配置环境变量" class="heading-link"><i class="fas fa-link"></i></a><a href="#4、配置环境变量" class="headerlink" title="4、配置环境变量"></a>4、配置环境变量</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile </span><br><span class="line"></span><br><span class="line">export KAFKA_HOME=/export/server/kafka </span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin </span><br><span class="line">source /etc/profile </span><br><span class="line">#注意:还需要分发环境变量</span><br><span class="line">syncfile /etc/profile</span><br></pre></td></tr></table></div></figure>


        <h2 id="5、分别在node2和node3上修改配置文件"   >
          <a href="#5、分别在node2和node3上修改配置文件" class="heading-link"><i class="fas fa-link"></i></a><a href="#5、分别在node2和node3上修改配置文件" class="headerlink" title="5、分别在node2和node3上修改配置文件"></a>5、分别在node2和node3上修改配置文件</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim /export/server/kafka/config/server.propertie</span><br><span class="line">broker.id=1 </span><br><span class="line">broker.id=2</span><br><span class="line">#(broker.id 不能重复)</span><br><span class="line"></span><br><span class="line">#启停集群(在各个节点上启动)</span><br><span class="line">#启动集群</span><br><span class="line">kafka-server-start.sh -daemon /export/server/kafka/config/server.properties </span><br><span class="line">#停止集群</span><br><span class="line">kafka-server-stop.sh stop</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/kafka/1.png"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/20/hexo+github/">使用hexo+GitHub建立个人博客</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-20</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-25</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="一、安装Hexo"   >
          <a href="#一、安装Hexo" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、安装Hexo" class="headerlink" title="一、安装Hexo"></a>一、安装Hexo</h1>
      <p>安装 Hexo 时先安装下列应用程序：</p>
<ul>
<li>Node.js (Node.js 版本需不低于 10.13，建议使用 Node.js 12.0 及以上版本)</li>
<li>Git</li>
</ul>

        <h2 id="（1）git"   >
          <a href="#（1）git" class="heading-link"><i class="fas fa-link"></i></a><a href="#（1）git" class="headerlink" title="（1）git"></a>（1）git</h2>
      
        <h3 id="git下载"   >
          <a href="#git下载" class="heading-link"><i class="fas fa-link"></i></a><a href="#git下载" class="headerlink" title="git下载"></a>git下载</h3>
      <p>git的下载地址：<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://git-scm.com/download" >https://git-scm.com/download</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p>附件：</p>
<p><img src="/../image/hexo/1.png"></p>

        <h3 id="git安装"   >
          <a href="#git安装" class="heading-link"><i class="fas fa-link"></i></a><a href="#git安装" class="headerlink" title="git安装"></a>git安装</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1、按照附件的顺序，直接下一步安装即可。</span><br><span class="line">2、安装的过程中需要填写一个邮箱和用户名(任意即可)</span><br><span class="line">3、安装完毕请重启资源管理器,或者重启电脑</span><br><span class="line">4、更改语言(选择修改)</span><br></pre></td></tr></table></div></figure>


        <h2 id="（2）nodejs"   >
          <a href="#（2）nodejs" class="heading-link"><i class="fas fa-link"></i></a><a href="#（2）nodejs" class="headerlink" title="（2）nodejs"></a>（2）nodejs</h2>
      
        <h3 id="nodejs下载和安装"   >
          <a href="#nodejs下载和安装" class="heading-link"><i class="fas fa-link"></i></a><a href="#nodejs下载和安装" class="headerlink" title="nodejs下载和安装"></a>nodejs下载和安装</h3>
      <p>1、官方下载对应的系统安装包。</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://nodejs.org/en/download/</span><br></pre></td></tr></table></div></figure>

<p>2、安装时建议修改安装路径，放在非C盘目录下，一路默认安装即可。</p>
<p>3、安装完成后启动命令行工具，输入<code>node -v</code>和<code>npm -v</code>查看安装版本，出现提示版本信息即为安装成功。</p>
<p><img src="/../image/hexo/2.png"></p>

        <h3 id="环境配置"   >
          <a href="#环境配置" class="heading-link"><i class="fas fa-link"></i></a><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3>
      <p>1、在Node.js安装目录下，如 D:\nodejs 新建两个文件夹：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node_global(全局包存放目录)</span><br><span class="line"></span><br><span class="line">node_cache(缓存目录)</span><br></pre></td></tr></table></div></figure>

<p>2、打开命令行工具，执行以下两句操作：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#设置全局包目录</span><br><span class="line">npm config set prefix &quot;D:\nodejs\node_global&quot; </span><br><span class="line">#设置缓存目录</span><br><span class="line">npm config set cache &quot;D:\nodejs\node_cache&quot;</span><br></pre></td></tr></table></div></figure>

<p>3、配置环境变量：</p>
<p>（1）、打开环境变量，在系统变量中新建</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">变量名：NODE_PATH</span><br><span class="line">变量值：D:\nodejs\node_global\node_modules</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/hexo/3.png"></p>
<p>（2）、编辑用户变量的 path</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 将默认的 C 盘下 APPData\Roaming\npm 修改为 </span><br><span class="line">D:\nodejs\node_global</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/hexo/4.png"></p>

        <h3 id="切换镜像源"   >
          <a href="#切换镜像源" class="heading-link"><i class="fas fa-link"></i></a><a href="#切换镜像源" class="headerlink" title="切换镜像源"></a>切换镜像源</h3>
      <p>1、npm查看当前源：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm get registry</span><br></pre></td></tr></table></div></figure>

<p>2、npm设置淘宝镜像源：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm config set registry https://registry.npm.taobao.org</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/hexo/5.png"></p>

        <h2 id="（3）hexo"   >
          <a href="#（3）hexo" class="heading-link"><i class="fas fa-link"></i></a><a href="#（3）hexo" class="headerlink" title="（3）hexo"></a>（3）hexo</h2>
      <p>所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/hexo/6.png"></p>

        <h1 id="二、hexo搭建静态网页"   >
          <a href="#二、hexo搭建静态网页" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、hexo搭建静态网页" class="headerlink" title="二、hexo搭建静态网页"></a>二、hexo搭建静态网页</h1>
      <p>Hexo是基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Coding上，是搭建博客的首选框架。</p>
<p>本地访问博客</p>
<p>执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo init &lt;folder&gt;</span><br><span class="line">cd &lt;folder&gt;</span><br><span class="line">npm install     # 安装组件</span><br></pre></td></tr></table></div></figure>

<p>新建完成后，指定文件夹的目录如下：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── _config.yml</span><br><span class="line">├── package.json</span><br><span class="line">├── scaffolds</span><br><span class="line">├── source</span><br><span class="line">|   ├── _drafts</span><br><span class="line">|   └── _posts</span><br><span class="line">└── themes</span><br><span class="line"># _config.yml（网站的配置信息，可以在此配置大部分的参数）</span><br><span class="line"></span><br><span class="line"># source（资源文件夹是存放用户资源的地方。除 _posts 文件夹之外，开头命名为 _ (下划线)的文件 / 文件夹和隐藏的文件将会被忽略。Markdown 和 HTML 文件会被解析并放到 public 文件夹，而其他文件会被拷贝过去。）#其中md文档放在_posts文件夹下</span><br><span class="line"></span><br><span class="line"># themes（主题文件夹。Hexo 会根据主题来生成静态页面。）</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/hexo/7.png"></p>
<p><img src="/../image/hexo/8.png"></p>
<p>执行下列命令，启动本地服务器进行预览：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g   # 生成页面</span><br><span class="line">hexo s   # 启动预览</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/hexo/9.png"></p>
<p><img src="/../image/hexo/10.png"></p>
<p>访问 <code>http://localhost:4000</code>，出现 Hexo 默认页面，本地博客安装成功</p>
<p><img src="/../image/hexo/11.png"></p>

        <h1 id="三、通过ssh连接将博客部署到github"   >
          <a href="#三、通过ssh连接将博客部署到github" class="heading-link"><i class="fas fa-link"></i></a><a href="#三、通过ssh连接将博客部署到github" class="headerlink" title="三、通过ssh连接将博客部署到github"></a>三、通过ssh连接将博客部署到github</h1>
      <p>静态页面（本地博客）已经生成，还需要将其部署到GitHub上。一来搭建的博客别人可以（以一个链接）访问到，二来需要有个托管平台，这样可以关注博客内容本身而不是麻烦的管理。</p>

        <h2 id="创建github仓库"   >
          <a href="#创建github仓库" class="heading-link"><i class="fas fa-link"></i></a><a href="#创建github仓库" class="headerlink" title="创建github仓库"></a>创建github仓库</h2>
      <p>1、注册github账户</p>
<p><img src="/../image/hexo/12.png"></p>
<p>2、新建仓库</p>
<p>创建一个与用户名相同的仓库，后面加.github.io，只有这样，将来要部署到GitHub page的时候，才会被识别为xxxx.github.io，其中xxx就是注册GitHub的用户名。</p>
<p><img src="/../image/hexo/13.png"></p>

        <h2 id="ssh连接"   >
          <a href="#ssh连接" class="heading-link"><i class="fas fa-link"></i></a><a href="#ssh连接" class="headerlink" title="ssh连接"></a>ssh连接</h2>
      <p>在本地通过ssh绑定github仓库，可通过 git 上传文件到 github上。</p>
<p>1、在cmd中执行以下命令，生成公钥：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;yourname&quot;</span><br><span class="line">git config --global user.email &quot;youremail&quot;</span><br><span class="line"></span><br><span class="line">ssh-keygen -t rsa -C &quot;youremail&quot;</span><br></pre></td></tr></table></div></figure>

<p>2、将生成的公钥（id_rsa.pub的内容）复制github上</p>
<p><img src="/../image/hexo/17.png"></p>

        <h2 id="修改配置文件"   >
          <a href="#修改配置文件" class="heading-link"><i class="fas fa-link"></i></a><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2>
      <p>配置<folder>目录下的_config.yml</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 配置hexo把&lt;folder&gt;部署到github仓库里</span><br><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: git@github.com:username/username.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></div></figure>


        <h2 id="安装hexo部署插件"   >
          <a href="#安装hexo部署插件" class="heading-link"><i class="fas fa-link"></i></a><a href="#安装hexo部署插件" class="headerlink" title="安装hexo部署插件"></a>安装hexo部署插件</h2>
      <p>安装deploy-git ，也就是部署的命令,这样才能用命令部署到GitHub。</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></div></figure>


        <h2 id="通过github-page访问Hexo博客"   >
          <a href="#通过github-page访问Hexo博客" class="heading-link"><i class="fas fa-link"></i></a><a href="#通过github-page访问Hexo博客" class="headerlink" title="通过github page访问Hexo博客"></a>通过github page访问Hexo博客</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#清除生成的网页文件</span><br><span class="line">hexo clean</span><br><span class="line">#生成网页文件</span><br><span class="line">hexo g</span><br><span class="line">#上传网页文件到 Github page</span><br><span class="line">hexo d</span><br></pre></td></tr></table></div></figure>

<p>接着访问<code>https://username.github.io/</code>即可</p>

        <h1 id="四、修改主题"   >
          <a href="#四、修改主题" class="heading-link"><i class="fas fa-link"></i></a><a href="#四、修改主题" class="headerlink" title="四、修改主题"></a>四、修改主题</h1>
      <p>系统默认给的主题是 landscape，将主题修改为stun</p>
<p>1、进入你的 hexo 站点文件夹，克隆 <code>stun</code> 主题到 <code>themes/</code> 路径下</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd &lt;folder&gt;</span><br><span class="line">git clone https://github.com/liuyib/hexo-theme-stun.git themes/stun</span><br></pre></td></tr></table></div></figure>

<p>2、安装依赖 <code>hexo-renderer-pug</code></p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-renderer-pug</span><br></pre></td></tr></table></div></figure>

<p>3、修改<folder>目录下的_config.yml文件</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theme: stun</span><br></pre></td></tr></table></div></figure>

<p>4、启动 Hexo 服务器</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#清除生成的网页文件</span><br><span class="line">hexo clean</span><br><span class="line">#生成网页文件</span><br><span class="line">hexo g</span><br><span class="line">#上传网页文件到 Github page</span><br><span class="line">hexo d</span><br></pre></td></tr></table></div></figure>

<p>5、访问<code>https://username.github.io/</code></p>

        <h1 id="五、上传md文档"   >
          <a href="#五、上传md文档" class="heading-link"><i class="fas fa-link"></i></a><a href="#五、上传md文档" class="headerlink" title="五、上传md文档"></a>五、上传md文档</h1>
      <p>1、在<folder>项目中，source文件夹下的_posts文件夹中上传md文档</p>
<p><img src="/../image/hexo/14.png"></p>
<p>2、md文档中的图片可以放在source文件夹下新建的image文件夹中</p>
<p><img src="/../image/hexo/15.png"></p>
<p>注：每次上传后都要执行以下命令，否则通过github page访问Hexo博客中的内容不更新</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#清除生成的网页文件</span><br><span class="line">hexo clean</span><br><span class="line">#生成网页文件</span><br><span class="line">hexo g</span><br><span class="line">#上传网页文件到 Github page</span><br><span class="line">hexo d</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/hexo/16.png"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/14/spark%E5%AE%9E%E8%AE%AD/">saprk</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-14</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-25</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="1、安装nodejs"   >
          <a href="#1、安装nodejs" class="heading-link"><i class="fas fa-link"></i></a><a href="#1、安装nodejs" class="headerlink" title="1、安装nodejs"></a>1、安装nodejs</h1>
      <p><img src="/..%5Cimage%5Cgit%5C1.png"></p>
</div></div></article></section><nav class="paginator"><div class="paginator-inner"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fas fa-angle-right"></i></a></div></nav></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><section class="sidebar-toc hide"></section><!-- ov = overview--><section class="sidebar-ov"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/stun-logo.svg" alt="avatar"></div><p class="sidebar-ov-author__text">Hello Stun</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">18</div><div class="sidebar-ov-state-item__name">Archives</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="Creative Commons" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2023</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>John Doe</span></div><div><span>Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a></span><span> v6.3.0</span><span class="footer__devider">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.8.0</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.8.0"></script><script src="/js/stun-boot.js?v=2.8.0"></script><script src="/js/scroll.js?v=2.8.0"></script><script src="/js/header.js?v=2.8.0"></script><script src="/js/sidebar.js?v=2.8.0"></script></body></html>